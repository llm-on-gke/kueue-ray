# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START gke_ai_ml_gke_ray_raytrain_dws_rayjob_dist]
apiVersion: ray.io/v1
kind: RayJob
metadata:
  #labels:
  #   kueue.x-k8s.io/queue-name: dws-local-queue
  name: pytorch-mnist-job
spec:
  #suspend: true
  shutdownAfterJobFinishes: true
  entrypoint: python ai-ml/gke-ray/raytrain/dws/train.py
  runtimeEnvYAML: |
    pip:
      - torch
      - torchvision
    working_dir: "https://github.com/rick-c-goog/kubernetes-engine-samples/archive/ray-train.zip"
    env_vars:
     NUM_WORKERS: "16"
     CPUS_PER_WORKER: "80"
     GPUS_PER_WORKER: "1"
  rayClusterSpec:
   rayVersion: '2.41.0'
   headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            - name: ray-head
              image: rayproject/ray:2.44.1-py39-cu128 # rayproject/ray-ml:2.41.0.021baf-py39 #
              env:
              - name: NUM_WORKERS
                value: "16"
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
              resources:
                limits:
                  cpu: "1"
                  memory: "5G"
                  #nvidia.com/gpu: "1"
                requests:
                  cpu: "1"
                  memory: "5G"
                  #nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
          volumes:
            - name: ray-logs
              emptyDir: {}
   workerGroupSpecs:
   - replicas: 2
     minReplicas: 2
     maxReplicas: 4
     groupName: gpu-group
     rayStartParams: {}
     template:
      metadata:
        annotations:
          #kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
          networking.gke.io/default-interface: 'eth0'
          networking.gke.io/interfaces: |
            [
              {"interfaceName":"eth0","network":"default"},
              {"interfaceName":"eth1","network":"gvnic-1"},
              {"interfaceName":"eth2","network":"gvnic-2"},
              {"interfaceName":"eth3","network":"gvnic-3"},
              {"interfaceName":"eth4","network":"gvnic-4"},
              {"interfaceName":"eth5","network":"gvnic-5"},
              {"interfaceName":"eth6","network":"gvnic-6"},
              {"interfaceName":"eth7","network":"gvnic-7"},
              {"interfaceName":"eth8","network":"gvnic-8"}              
            ]
          devices.gke.io/container.tcpxo-daemon: |+
            - path: /dev/nvidia0
            - path: /dev/nvidia1
            - path: /dev/nvidia2
            - path: /dev/nvidia3
            - path: /dev/nvidia4
            - path: /dev/nvidia5
            - path: /dev/nvidia6
            - path: /dev/nvidia7
            - path: /dev/nvidiactl
            - path: /dev/nvidia-uvm
            - path: /dev/dmabuf_import_helper
      spec:
        nodeSelector:
            cloud.google.com/gke-nodepool: a3-megagpu-8g-a3megagpupool

        tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
        containers:
        - name: ray-worker
          image: rayproject/ray:2.44.1-py39-cu128 #rayproject/ray-ml:2.41.0.021baf-py39 #
          resources:
            limits:
              cpu: "100"
              memory: "600Gi"
              nvidia.com/gpu: "8"
            requests:
              cpu: "100"
              memory: "600Gi"
              nvidia.com/gpu: "8"
          env:
          - name: LD_LIBRARY_PATH
            value: /usr/local/nvidia/lib64
          - name: TORCH_DISTRIBUTED_DEBUG
            value: "INFO"
          - name: NCCL_LIB_DIR
            value: /usr/local/nvidia/lib64
          - name: NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY
            value: /dev/aperture_devices
          - name: NCCL_SOCKET_IFNAME
            value: "eth0"
          - name: GLOO_SOCKET_IFNAME
            value: "eth0"
          - name: NCCL_NSOCKS_PERTHREAD
            value: "4"
          - name: NCCL_SOCKET_NTHREADS
            value: "1"
          - name: NCCL_BUFFSIZE
            value: "8388608"
          - name: NCCL_DYNAMIC_CHUNK_SIZE
            value: "524288"
          - name: NCCL_ALGO
            value: Ring,Tree
          - name: NCCL_PROTO
            value: Simple
          - name: NCCL_CROSS_NIC
            value: "0"
          - name: NCCL_NET_GDR_LEVEL
            value: PIX
          - name: NCCL_P2P_NET_CHUNKSIZE
            value: "524288"
          - name: NCCL_P2P_PCI_CHUNKSIZE
            value: "524288"
          - name: NCCL_P2P_NVL_CHUNKSIZE
            value: "1048576"
          - name: NCCL_NVLS_ENABLE
            value: "0"
          - name: NCCL_FASTRAK_CTRL_DEV
            value: eth0
          - name: NCCL_FASTRAK_IFNAME
            value: eth1,eth2,eth3,eth4,eth5,eth6,eth7,eth8
          - name: NCCL_MIN_NCHANNELS
            value: "4"
          - name: NCCL_TUNER_PLUGIN
            value: libnccl-tuner.so
          - name: NCCL_TUNER_CONFIG_PATH
            value: /usr/local/nvidia/lib64/a3plus_tuner_config.textproto
          - name: NCCL_SHIMNET_GUEST_CONFIG_CHECKER_CONFIG_FILE
            value: /usr/local/nvidia/lib64/a3plus_guest_config.textproto
          - name: NCCL_FASTRAK_NUM_FLOWS
            value: "2" 
          - name: NCCL_FASTRAK_USE_SNAP
            value: "1"
          - name: NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS
            value: "600000"
          - name: NCCL_FASTRAK_ENABLE_CONTROL_CHANNEL
            value: "0"
          - name: CUDA_VISIBLE_DEVICES
            value: "0,1,2,3,4,5,6,7"
          - name: NCCL_FASTRAK_ENABLE_HOTPATH_LOGGING
            value: "0"
          - name: NCCL_FASTRAK_USE_LLCM
            value: "1" 
          - name: NCCL_DEBUG
            value: INFO # Or "WARN", "DEBUG", "TRACE" for more verbosity
          - name: NCCL_DEBUG_SUBSYS
            value: INIT,NET,ENV,COLL,GRAPH
          - name: NVTE_FWD_LAYERNORM_SM_MARGIN
            value: "8"
          - name: NVTE_BWD_LAYERNORM_SM_MARGIN
            value: "8"
          - name: NCCL_P2P_PXN_LEVEL
            value: "0"
          volumeMounts:
          - name: aperture-devices
            mountPath: /dev/aperture_devices
          - name: dshm
            mountPath: /dev/shm
        - name: tcpxo-daemon
          image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.15
          imagePullPolicy: Always
          command: ["/bin/sh", "-c"]
          args:
            - |
              set -ex
              chmod 755 /fts/entrypoint_rxdm_container.sh
              /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr &
              while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
                pkill -e "^"tcpgpudmarxd || true
                sleep 15
              exit 0
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - NET_BIND_SERVICE
          volumeMounts:
            - name: nvidia-install-dir-host
              mountPath: /usr/local/nvidia
            - name: nvidia
              mountPath: /usr/local/nvidia/lib64
            - name: sys
              mountPath: /hostsysfs
            - name: proc-sys
              mountPath: /hostprocsysfs
            - name: workload-terminated-volume
              mountPath: /semaphore
          env:
            - name: LD_LIBRARY_PATH
              value: /usr/local/nvidia/lib64
        #tolerations:
        #  - key: "nvidia.com/gpu"
        #     operator: "Exists"
        #    effect: "NoSchedule"
        volumes:
          - name: nvidia-dir-host
            hostPath:
              path: /home/kubernetes/bin/nvidia 
          - name: sys
            hostPath:
              path: /sys
          - name: proc-sys
            hostPath:
              path: /proc/sys
          - name: aperture-devices
            hostPath:
              path: /dev/aperture_devices
          - name: nvidia-install-dir-host
            hostPath:
              path: /home/kubernetes/bin/nvidia
          - name: nvidia
            hostPath:
              path: /home/kubernetes/bin/nvidia/lib64
          - name: local-ssd
            hostPath:
              path: /mnt/stateful_partition/kube-ephemeral-ssd
          - name: dshm
            emptyDir:
              medium: Memory
          - name: shared-memory
            emptyDir:
              medium: "Memory"
              sizeLimit: 250Gi
          - name: workload-terminated-volume
            emptyDir: {}

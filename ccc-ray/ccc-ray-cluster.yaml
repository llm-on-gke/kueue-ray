apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: ray-cluster-ccc
spec:
  enableInTreeAutoscaling: true
  autoscalerOptions:
    idleTimeoutSeconds: 7200 # warm period before node is shut down
  rayVersion: '2.44.1'
  headGroupSpec:
    rayStartParams:
      dashboard-host: '0.0.0.0'
    template:
      metadata:
        annotations:
          gke-gcsfuse/volumes: "true"
          gke-gcsfuse/cpu-limit: "0"
          gke-gcsfuse/memory-limit: "0"
          gke-gcsfuse/ephemeral-storage-limit: "0"
      spec:
        restartPolicy: Never
        serviceAccountName: "nearmap-ray"
        nodeSelector:
          cloud.google.com/gke-nodepool: "default-pool"
        containers:
        - name: ray-head
          image: us-south1-docker.pkg.dev/nm-ai-sandbox/ray/ray:2.44.1-py39-cu128
          env:
          - name: RAY_enable_autoscaler_v2
            value: "1"
          ports:
          - containerPort: 6379
            name: gcs-server
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          resources:
            limits:
              cpu: "8"
              memory: "16G"
              ephemeral-storage: "9Gi"
            requests:
              cpu: "8"
              memory: "16G"
              ephemeral-storage: "9Gi"
          volumeMounts:
            - name: ray-logs
              mountPath: /tmp/ray
            - name: gcs-fuse-csi-eph
              mountPath: /bucket
        volumes:
          - name: ray-logs
            emptyDir: {}
          - name: gcs-fuse-csi-eph
            csi:
              driver: gcsfuse.csi.storage.gke.io
              volumeAttributes:
                bucketName: nearmap-ray
                gcsfuseMetadataPrefetchOnMount: "true"
                mountOptions: "implicit-dirs,file-mode=777,dir-mode=777,file-cache:enable-parallel-downloads:true,file-cache:cache-file-for-range-read:true,file-cache:max-size-mb:-1,write:enable-streaming-writes:true,read_ahead_kb=1024,file-system:kernel-list-cache-ttl-secs:-1"
  workerGroupSpecs:
  - replicas: 2
    minReplicas: 0
    maxReplicas: 10
    groupName: gpu-group
    rayStartParams:
      num-cpus: "220"
    template:
      metadata:
        annotations:
          gke-gcsfuse/volumes: "true"
          gke-gcsfuse/cpu-limit: "0"
          gke-gcsfuse/memory-limit: "0"
          gke-gcsfuse/ephemeral-storage-limit: "0"
          networking.gke.io/default-interface: 'eth0'
          networking.gke.io/interfaces: |
            [
              {"interfaceName":"eth0","network":"default"},
              {"interfaceName":"eth1","network":"gvnic-1"},
              {"interfaceName":"eth2","network":"rdma-0"},
              {"interfaceName":"eth3","network":"rdma-1"},
              {"interfaceName":"eth4","network":"rdma-2"},
              {"interfaceName":"eth5","network":"rdma-3"},
              {"interfaceName":"eth6","network":"rdma-4"},
              {"interfaceName":"eth7","network":"rdma-5"},
              {"interfaceName":"eth8","network":"rdma-6"},
              {"interfaceName":"eth9","network":"rdma-7"}
            ]
      spec:
        restartPolicy: Never
        serviceAccountName: "nearmap-ray"
        nodeSelector:
          cloud.google.com/compute-class: "h200-ccc"
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        containers:
        - name: ray-worker
          image: us-south1-docker.pkg.dev/nm-ai-sandbox/ray/ray:2.44.1-py39-cu128
          env:
          - name: LD_LIBRARY_PATH
            value: /usr/local/nvidia/lib64
          command: ["source /usr/local/gib/scripts/set_nccl_env.sh"]
          resources:
            limits:
              cpu: "220"
              memory: "2800Gi"
              nvidia.com/gpu: "8"
              ephemeral-storage: "1000Gi"
            requests:
              cpu: "220"
              memory: "2800Gi"
              nvidia.com/gpu: "8"
              ephemeral-storage: "1000Gi"
          volumeMounts:
          - name: nvidia
            mountPath: /usr/local/nvidia
          - name: gib
            mountPath: /usr/local/gib
          - name: shared-memory
            mountPath: /dev/shm
          - name: ray-tmp-storage
            mountPath: /tmp
          - name: gcs-fuse-csi-eph
            mountPath: /bucket
        volumes:
        - name: gib
          hostPath:
            path: /home/kubernetes/bin/gib
        - name: nvidia
          hostPath:
            path: /home/kubernetes/bin/nvidia
        - name: lib64
          hostPath:
            path: /lib64
        - name: shared-memory
          emptyDir:
            medium: "Memory"
            sizeLimit: 250Gi
        - name: sys
          hostPath:
            path: /sys
        - name: proc-sys
          hostPath:
            path: /proc/sys
        - name: ray-tmp-storage
          emptyDir: {}
        - name: gcs-fuse-csi-eph
          csi:
            driver: gcsfuse.csi.storage.gke.io
            volumeAttributes:
              bucketName: nearmap-ray
              gcsfuseMetadataPrefetchOnMount: "true"
              mountOptions: "implicit-dirs,file-mode=777,dir-mode=777,file-cache:enable-parallel-downloads:true,file-cache:cache-file-for-range-read:true,file-cache:max-size-mb:-1,write:enable-streaming-writes:true,read_ahead_kb=1024,file-system:kernel-list-cache-ttl-secs:-1"
---
apiVersion: v1
kind: Service
metadata:
  name: ray-dashboard-service
spec:
  type: NodePort
  ports:
    - port: 8265
      targetPort: dashboard
      protocol: TCP
      name: dashboard
    - port: 10001
      targetPort: client
      protocol: TCP
      name: client
  selector:
    ray.io/cluster: ray-cluster-ccc
    ray.io/node-type: head
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ray-dashboard-internal-ingress
  annotations:
    kubernetes.io/ingress.class: "gce-internal" # remove to make into external applicaiotn load balancer
spec:
  # ingressClassName: gce # for external application load balancer
  rules:
  - http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: ray-dashboard-service
            port:
              name: dashboard
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: pytorch-mnist-cluster
  labels:
    kueue.x-k8s.io/queue-name: dws-local-queue
  annotations:
    provreq.kueue.x-k8s.io/maxRunDurationSeconds: "41600" # Max run time in sec
spec:
  suspend: true #true ensures all pods start together
  rayVersion: '2.47.0'
  headGroupSpec:
    rayStartParams:
      dashboard-host: '0.0.0.0'
    template:
      spec:
        nodeSelector:
          cloud.google.com/gke-nodepool: "default-pool"
        containers:
        - name: ray-head
          image: rayproject/ray-ml:2.9.0 #rayproject/ray:2.47.0
          ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
          resources:
            limits:
              cpu: "0.5"
              memory: "2G"
              ephemeral-storage: "9Gi"
            requests:
              cpu: "0.5"
              memory: "2G"
              ephemeral-storage: "9Gi"
          volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
        volumes:
          - name: ray-logs
            emptyDir: {}
  workerGroupSpecs:
  - replicas: 4
   
    minReplicas: 4
    maxReplicas: 4
    numOfHosts: 1
    groupName: tpu-group
    rayStartParams: {}
      #num-cpus: "220"
    template:
      metadata:
        labels:
          kueue.x-k8s.io/queue-name: dws-local-queue
        annotations: {}
          #provreq.kueue.x-k8s.io/maxRunDurationSeconds: "41600" # Max run time in sec
          #kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
          
      spec:
        nodeSelector:
          cloud.google.com/gke-nodepool: "v6e-4-4-ray-dws"
          cloud.google.com/gke-tpu-accelerator: tpu-v6e-slice
          cloud.google.com/gke-tpu-topology: 2x2
        #nodeSelector:
        #  cloud.google.com/gke-flex-start: "true"
        #tolerations:
        #  - key: "nvidia.com/gpu"
        #    operator: "Exists"
        #    effect: "NoSchedule"
        containers:
        - name: ray-worker
          image: rayproject/ray-ml:2.9.0 #rayproject/ray:2.47.0-gpu
          env:
           - name: LD_LIBRARY_PATH
             value: /usr/local/nvidia/lib64
           
          resources:
            limits:
                cpu: "100"
                google.com/tpu: "4"
                ephemeral-storage: 40G
                memory: 200G
            requests:
                cpu: "100"
                google.com/tpu: "4"
                ephemeral-storage: 40G
                memory: 200G
          